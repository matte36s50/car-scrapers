name: BAT and CNB Scraper Pipeline
on:
  schedule:
    - cron: '0 6 * * *'   # 6 AM UTC daily
    - cron: '0 18 * * *'  # 6 PM UTC daily
  workflow_dispatch:      # Allow manual trigger
jobs:
  scrape-and-analyze:
    runs-on: ubuntu-22.04
    timeout-minutes: 180
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
        fetch-depth: 1
    
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install playwright==1.40.0
        pip install beautifulsoup4==4.12.2
        pip install pandas==2.1.4
        pip install boto3==1.34.0
        pip install lxml==4.9.3
        pip install requests==2.31.0
        pip install numpy==1.24.4
    
    - name: Install Playwright with all dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libnss3 libnspr4 libatk1.0-0 libatk-bridge2.0-0 libcups2 libdrm2 libdbus-1-3 libatspi2.0-0 libx11-6 libxcomposite1 libxdamage1 libxext6 libxfixes3 libxrandr2 libgbm1 libxcb1 libxkbcommon0 libpango-1.0-0 libcairo2 libasound2
        python -m playwright install chromium
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-2
    
    - name: Test S3 connection
      run: |
        echo "Testing S3 connection to my-mii-reports bucket..."
        aws s3 ls s3://my-mii-reports/ --max-items 5 || echo "S3 connection test completed"
    
    - name: Run BAT scraper
      timeout-minutes: 60
      run: |
        echo "Starting BAT scraper..."
        python -c "print('Python is working')"
        python -c "import pandas; print('Pandas OK')"
        python -c "import boto3; print('Boto3 OK')"
        python -c "from playwright.sync_api import sync_playwright; print('Playwright sync_api OK')"
        echo "All imports verified, running scraper..."
        python -u bat_scraper.py
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: us-east-2
    
    - name: Run CNB scraper
      if: success()
      timeout-minutes: 15
      run: |
        echo "Starting CNB scraper..."
        python cnb_scraper.py || echo "CNB scraper completed with issues"
      continue-on-error: true
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: us-east-2
    
    - name: Run MII calculator
      if: success()
      timeout-minutes: 10
      run: |
        echo "Starting MII calculator..."
        python enhanced_mii_all_models.py || echo "MII calculator completed"
      continue-on-error: true
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: us-east-2
    
    - name: Upload artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-outputs-${{ github.run_number }}
        path: |
          *.csv
          *.json
          *.log
        retention-days: 7
        if-no-files-found: warn
    
    - name: Check outputs
      if: always()
      run: |
        echo "Checking generated files:"
        ls -la *.csv 2>/dev/null || echo "No CSV files"
        ls -la *.json 2>/dev/null || echo "No JSON files"
