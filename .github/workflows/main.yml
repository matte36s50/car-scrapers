name: Daily Car Scraping

on:
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libnss3-dev \
          libatk-bridge2.0-dev \
          libdrm-dev \
          libxcomposite-dev \
          libxdamage-dev \
          libxrandr-dev \
          libgbm-dev \
          libxss-dev \
          xvfb
    
    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Install Playwright browsers (with fallback)
      run: |
        echo "Installing Playwright browsers..."
        playwright install chromium || echo "Playwright install failed, trying alternative..."
        
        # Try installing with system dependencies
        sudo apt-get install -y chromium-browser || echo "System chromium install failed"
        
        # Verify installation
        playwright --version || echo "Playwright CLI not available"
        
        echo "Browser installation completed (check logs for any issues)"
    
    - name: Configure AWS
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}
    
    - name: Start virtual display
      run: |
        export DISPLAY=:99
        Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
        sleep 3
        echo "Virtual display started"
    
    - name: Run CNB Scraper
      run: |
        export DISPLAY=:99
        echo "ğŸš€ Starting CNB scraper..."
        python cnb_scraper.py
        echo "âœ… CNB scraper completed successfully"
      continue-on-error: true
      
    - name: Wait between scrapers
      run: |
        echo "â³ Waiting 30 seconds between scrapers..."
        sleep 30
      
    - name: Run BAT Scraper
      run: |
        export DISPLAY=:99
        echo "ğŸš€ Starting BAT scraper..."
        python bat_scraper.py
        echo "âœ… BAT scraper completed successfully"
      continue-on-error: true
    
    - name: Run Mii Calculations
      run: |
        export DISPLAY=:99
        echo "ğŸ§® Starting Mii calculations..."
        python enhanced_mii_all_models.py
        echo "âœ… Mii calculations completed successfully"
      continue-on-error: true
    
    - name: Upload CSV files to S3
      run: |
        echo "â˜ï¸ Uploading all CSV files to S3..."
        uploaded_count=0
        
        for file in *.csv; do
          if [ -f "$file" ]; then
            echo "ğŸ“„ Uploading: $file"
            if aws s3 cp "$file" s3://my-mii-reports/; then
              echo "âœ… Successfully uploaded: $file"
              uploaded_count=$((uploaded_count + 1))
            else
              echo "âŒ Failed to upload: $file"
            fi
          fi
        done
        
        # Also upload JSON reports if they exist
        for file in *.json; do
          if [ -f "$file" ]; then
            echo "ğŸ“„ Uploading JSON: $file"
            if aws s3 cp "$file" s3://my-mii-reports/; then
              echo "âœ… Successfully uploaded: $file"
              uploaded_count=$((uploaded_count + 1))
            else
              echo "âŒ Failed to upload: $file"
            fi
          fi
        done
        
        if [ $uploaded_count -eq 0 ]; then
          echo "âš ï¸ No files found to upload"
        else
          echo "ğŸ‰ Uploaded $uploaded_count file(s) to S3"
        fi
        
        # List recent files in S3 bucket
        echo "ğŸ“‚ Recent files in S3 bucket:"
        aws s3 ls s3://my-mii-reports/ --recursive | sort -k1,2 | tail -15
    
    - name: Upload artifacts (for debugging)
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: scraping-results
        path: |
          *.csv
          *.log
          *.json
        retention-days: 7
