name: Daily Car Scraping

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch: # Allows manual run

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Install Playwright browsers
      run: |
        playwright install chromium
        playwright install-deps chromium
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}
    
    - name: Run CNB Scraper
      run: |
        echo "Starting CNB scraper..."
        python cnb_scraper.py
        echo "CNB scraper completed"
      continue-on-error: true
      
    - name: Wait between scrapers
      run: sleep 30
      
    - name: Run BAT Scraper
      run: |
        echo "Starting BAT scraper..."
        python bat_scraper.py
        echo "BAT scraper completed"
      continue-on-error: true
    
    - name: Upload CSV files to S3
      run: |
        if ls *.csv 1> /dev/null 2>&1; then
          for file in *.csv; do
            aws s3 cp "$file" s3://my-mii-reports/
            echo "Uploaded $file to S3"
          done
        else
          echo "No CSV files found to upload"
        fi
      continue-on-error: true
    
    - name: Upload artifacts (for debugging)
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: scraping-results
        path: |
          *.csv
          *.log
        retention-days: 7